---
description: Rigorous test scenario generation during planning
alwaysApply: true
---

# Plan Mode: Test Scenario Generation

When planning ANY task, generate comprehensive test scenarios that guarantee 100% coverage of requirements. Scenarios become acceptance criteria — the task is only done when all pass (see `02-task-completion.mdc`, `00-core.mdc` → Evidence). Align test philosophy and naming with `11-testing.mdc`.

## Scope: Full vs Minimal Scenarios

- **Full section (mandatory):** Multi-step tasks, new or changed user-facing behavior, API changes, bug fixes with non-obvious causes. Use the full Test Scenarios section below.
- **Minimal (acceptable):** Trivial changes (typo, config value, comment, single-line fix with no behavior change). A short list of 1–3 scenarios or “no new scenarios; existing tests cover change” is enough.

## Mandatory Test Scenarios Section

Every non-trivial plan MUST include a **Test Scenarios** section with:

### 1. Requirement Traceability Matrix
Map each requirement to specific test scenarios:

| Requirement | Test Scenario | Type | Priority |
|-------------|---------------|------|----------|
| User can create X | `should create X with valid data` | Integration | P0 |
| X validates email | `should reject invalid email format` | Unit | P0 |

### 2. Priority Definitions
- **P0 (must have):** Critical path, core behavior, failure = broken feature. Every requirement has at least one P0 scenario.
- **P1 (should have):** Security, integration, important edge cases. Include unless scope is minimal.
- **P2 (nice to have):** Performance, accessibility (when UI), backward compatibility. Add when task touches that area.

### 3. Test Type Selection (see `11-testing.mdc`)
- **Integration (preferred):** Real DB/services; behavior of the unit under test with real dependencies.
- **E2E:** Full user flows, critical journeys, UI + API together.
- **Unit (fallback):** Pure functions only (validation, formatters, mappers). Do not unit-test glue code that only delegates.

Choose type per scenario so the plan is implementable with the project’s testing philosophy.

### 4. Test Categories (All Required Where Applicable)

#### Happy Path (P0)
- All success scenarios with valid inputs
- Expected behavior and state changes
- One scenario per distinct success outcome

#### Edge Cases (P0)
- Boundary values (min, max, empty, null, zero)
- Special characters and Unicode
- Large data sets / limits (e.g. max length, pagination)

#### Error Handling (P0)
- Invalid inputs (wrong type, format, range)
- Missing required fields
- Unauthorized access attempts
- Resource not found (404, etc.)

#### Security (P1)
- Authentication/authorization bypass attempts
- Injection (SQL, XSS, etc.) and input sanitization
- Rate limiting when the task touches APIs or auth

#### Integration Points (P1)
- External service failures (see `08-external-services.mdc`)
- Timeout and retry behavior
- Contract/response shape when APIs change

#### Optional by Task Type
- **UI/frontend:** Accessibility scenarios (keyboard, screen reader, focus) — see `16-accessibility.mdc`.
- **API change:** Backward compatibility or explicit breaking-change scenario.
- **Background jobs:** Idempotency, duplicate handling — see `24-background-jobs.mdc`.

### 5. Scenario Template

```markdown
## Test Scenario: [Descriptive Name]

**Requirement:** [Which requirement this tests]
**Type:** Unit | Integration | E2E
**Priority:** P0 | P1 | P2

**Preconditions:**
- [Setup required; avoid order-dependent or time-dependent setup]

**Steps:**
1. Given [initial state]
2. When [action]
3. Then [expected result]

**Assertions:**
- [ ] [Specific check 1]
- [ ] [Specific check 2]
```

### 6. Naming and Quality Rules (align with `11-testing.mdc`)
- **Names:** Describe behavior, not implementation. Use `should [behavior]` style: `should create user with valid email`, `should return 400 for invalid format`.
- **NEVER** put ticket/task IDs in scenario names or describe blocks.
- **Assertions:** Specific and measurable (status code, field value, side effect). No vague checks like “works correctly”.
- **Flaky prevention:** No reliance on test order or wall-clock time; prefer explicit setup/teardown and stable data.

## Completeness Checklist

Before finalizing the plan, verify:

- [ ] Every requirement has at least one test scenario (traceability complete)
- [ ] All happy paths covered
- [ ] All error scenarios documented
- [ ] Edge cases identified
- [ ] Security implications addressed (when auth/input/API)
- [ ] Integration points tested (when external services)
- [ ] Scenarios are implementable (concrete steps and assertions)
- [ ] Naming matches `11-testing.mdc`; no ticket IDs in names
- [ ] No requirement left untested

## Example Output

```markdown
## Test Scenarios for: Add user email validation

### Requirement Traceability
| Requirement | Scenarios |
|-------------|-----------|
| Validate email format | 3 scenarios |
| Reject duplicates | 2 scenarios |
| Handle edge cases | 4 scenarios |

### Happy Path
1. `should accept valid email format (user@domain.com)`
2. `should accept email with subdomain (user@sub.domain.com)`
3. `should accept email with plus sign (user+tag@domain.com)`

### Edge Cases
4. `should reject empty email`
5. `should reject email without @`
6. `should reject email without domain`
7. `should handle maximum length email (254 chars)`

### Error Handling
8. `should return 400 for invalid format`
9. `should return 409 for duplicate email`

### Security
10. `should sanitize email to prevent injection`
```

### Example: One Full Scenario (template in use)

```markdown
## Test Scenario: Reject duplicate email

**Requirement:** Reject duplicates
**Type:** Integration
**Priority:** P0

**Preconditions:**
- DB has user with email `existing@example.com`

**Steps:**
1. Given existing user above
2. When creating user with same email
3. Then API returns 409 and no second user is created

**Assertions:**
- [ ] Response status is 409
- [ ] Response body indicates duplicate email
- [ ] User table still has exactly one row with that email
```

## Enforcement

**Do NOT proceed with implementation until test scenarios are complete and approved.**

Test scenarios serve as acceptance criteria — the task is only done when all scenarios pass (see `02-task-completion.mdc`). Once implemented, these scenarios double as **regression tests** for that area: keep them in the suite so future changes do not break the behavior they describe.
